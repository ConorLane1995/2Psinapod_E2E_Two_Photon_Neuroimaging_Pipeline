{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_active_cells(deconvolved_traces):\n",
    "\n",
    "    # going to return a dictionary with only active cells, formatted exactly the same as traces\n",
    "\n",
    "    d = dict.fromkeys(deconvolved_traces.keys())\n",
    "\n",
    "    for cell in deconvolved_traces:\n",
    "        if deconvolved_traces[cell]['active'] == True:\n",
    "            d[cell] = deconvolved_traces[cell]\n",
    "        else:\n",
    "            d.pop(cell,None)\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH_1 = \"F:/Two-Photon/Psilocybin Project/Evoked Cohort Mice/ID473/psilo/TSeries-10132023-1324-139/suite2p/plane0/\"\n",
    "BASE_PATH_2 = \"F:/Two-Photon/Psilocybin Project/Evoked Cohort Mice/ID473/psilo/TSeries-10132023-1324-140/suite2p/plane0/\"\n",
    "file_path = \"F:/Two-Photon/Psilocybin Project/Evoked Cohort Mice/ID473/psilo/TSeries-10132023-1324-140/suite2p/plane0/roimatch_139_140.csv\"\n",
    "\n",
    "iscellpath_1 = BASE_PATH_1 + \"iscell.npy\"\n",
    "tracepath_1 = BASE_PATH_1 + \"F.npy\"\n",
    "cell_dict_1 = BASE_PATH_1 + \"cells.pkl\"\n",
    "\n",
    "iscellpath_2 = BASE_PATH_2 + \"iscell.npy\"\n",
    "tracepath_2 = BASE_PATH_2 + \"F.npy\"\n",
    "cell_dict_2 = BASE_PATH_2 + \"cells.pkl\"\n",
    "\n",
    "\n",
    "iscell_1 = np.load(iscellpath_1)\n",
    "traces_1 = np.load(tracepath_1)\n",
    "\n",
    "iscell_2 = np.load(iscellpath_2)\n",
    "traces_2 = np.load(tracepath_2)\n",
    "\n",
    "\n",
    "with open(cell_dict_1, 'rb') as f:\n",
    "    cell_dict_1 = pickle.load(f)\n",
    "\n",
    "with open(cell_dict_2, 'rb') as f:\n",
    "    cell_dict_2 = pickle.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROI_indices_1 = (iscell_1[:,0] == 1).nonzero()\n",
    "ROI_indices_1 = ROI_indices_1[0] # extracting the first part of the tuple\n",
    "cell_IDs_1 = ROI_indices_1 + 1 # add 1 so we don't have zero indexing\n",
    "\n",
    "# print(cell_IDs_1[436])\n",
    "\n",
    "ROI_indices_2 = (iscell_2[:,0] == 1).nonzero()\n",
    "ROI_indices_2 = ROI_indices_2[0] # extracting the first part of the tuple\n",
    "cell_IDs_2 = ROI_indices_2 + 1 # add 1 so we don't have zero indexing\n",
    "\n",
    "# print(cell_IDs_2[255])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 252 matched cells in first and 252 in second recording (should be the same)\n",
      "99 active cells detected in pre- recording\n",
      "99 active cells detected in post- recording\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load the CSV file into a NumPy array\n",
    "matched_cells = np.genfromtxt(file_path, delimiter=',', skip_header=0)\n",
    "matched_cells = matched_cells.astype(int) - 1  # Subtract 1 and convert to int in one step\n",
    "\n",
    "# Extract columns 0 and 1\n",
    "column_0 = matched_cells[:, 0]\n",
    "column_1 = matched_cells[:, 1]\n",
    "\n",
    "# Retrieve lists using NumPy indexing\n",
    "cell_array = np.zeros_like(matched_cells)\n",
    "cell_array[:,0] = cell_IDs_1[column_0]\n",
    "cell_array[:,1] = cell_IDs_2[column_1]\n",
    "\n",
    "print(\"Detected\", len(cell_array[:,0]), \"matched cells in first and\",len(cell_array[:,1]), \"in second recording (should be the same)\")\n",
    "\n",
    "\n",
    "active_1 = get_active_cells(cell_dict_1)\n",
    "\n",
    "matched_active_1 = [cell for cell in cell_array[:,0] if cell in active_1.keys()]\n",
    "\n",
    "print(len(matched_active_1), \"active cells detected in pre- recording\")\n",
    "\n",
    "active_2 = get_active_cells(cell_dict_2)\n",
    "\n",
    "matched_active_2 = [cell for cell in cell_array[:,1] if cell in active_2.keys()]\n",
    "\n",
    "print(len(matched_active_2), \"active cells detected in post- recording\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were 55 cells responsive in both recordings\n",
      "There were 44 cells responsive only in the first recording\n",
      "There were 44 cells responsive only in the second recording\n",
      "There were 143 cells responsive in at least one recording\n"
     ]
    }
   ],
   "source": [
    "#  Make a two column array containing each cell and it's matched cell. \n",
    "\n",
    "coactive_cells = []\n",
    "responded_in_minimum_one = []\n",
    "became_inactive = []\n",
    "became_active = []\n",
    "\n",
    "\n",
    "for i, (cell_1, cell_2) in enumerate(cell_array):\n",
    "    if cell_1 in matched_active_1 and cell_2 in matched_active_2:\n",
    "        coactive_cells.append(cell_array[i, :])\n",
    "    elif cell_1 in matched_active_1 and cell_2 not in matched_active_2:\n",
    "        became_inactive.append(cell_array[i,0])\n",
    "    elif cell_1 not in matched_active_1 and cell_2 in matched_active_2:\n",
    "        became_active.append(cell_array[i,0])\n",
    "    if cell_1 in matched_active_1 or cell_2 in matched_active_2:\n",
    "        responded_in_minimum_one.append(cell_array[i, :])\n",
    "\n",
    "# Convert the list of coactive_cells to a NumPy array\n",
    "coactive_cells_array = np.array(coactive_cells)\n",
    "active_only_first_array = np.array(became_inactive)\n",
    "active_only_second_array = np.array(became_active)\n",
    "active_in_minimum_one_array = np.array(responded_in_minimum_one)\n",
    "print(\"There were\", len(coactive_cells_array),\"cells responsive in both recordings\")\n",
    "print(\"There were\", len(became_inactive),\"cells responsive only in the first recording\")\n",
    "print(\"There were\", len(became_active),\"cells responsive only in the second recording\")\n",
    "print(\"There were\", len(responded_in_minimum_one),\"cells responsive in at least one recording\")\n",
    "\n",
    "np.save(BASE_PATH_2 + \"coactive_cells_array\",coactive_cells_array)\n",
    "np.save(BASE_PATH_2 + \"active_only_first\",active_only_first_array)\n",
    "np.save(BASE_PATH_2 + \"active_only_second\",active_only_first_array)\n",
    "np.save(BASE_PATH_2 + \"matched_cells\",cell_array)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   6   24]\n",
      " [  14   11]\n",
      " [  19    9]\n",
      " [  22   50]\n",
      " [  37   70]\n",
      " [  38   61]\n",
      " [  42   89]\n",
      " [  50   37]\n",
      " [  53   64]\n",
      " [  56   79]\n",
      " [  59   68]\n",
      " [  78   53]\n",
      " [  81  120]\n",
      " [  91   88]\n",
      " [  92   34]\n",
      " [  94   74]\n",
      " [  95  189]\n",
      " [  97  598]\n",
      " [ 117  374]\n",
      " [ 129  230]\n",
      " [ 132   84]\n",
      " [ 133  123]\n",
      " [ 136  178]\n",
      " [ 139  481]\n",
      " [ 143  136]\n",
      " [ 154  143]\n",
      " [ 170   30]\n",
      " [ 172  190]\n",
      " [ 182  126]\n",
      " [ 187  257]\n",
      " [ 198  107]\n",
      " [ 212  222]\n",
      " [ 216  278]\n",
      " [ 220  380]\n",
      " [ 226  812]\n",
      " [ 271  554]\n",
      " [ 281  579]\n",
      " [ 284  477]\n",
      " [ 311 1134]\n",
      " [ 320  375]\n",
      " [ 327   39]\n",
      " [ 354  137]\n",
      " [ 372  452]\n",
      " [ 453  292]\n",
      " [ 496  309]\n",
      " [ 503  102]\n",
      " [ 518  366]\n",
      " [ 535  408]\n",
      " [ 554  655]\n",
      " [ 599 1033]\n",
      " [ 654  484]\n",
      " [ 773  273]\n",
      " [ 866  180]\n",
      " [1069  290]\n",
      " [1651 1052]]\n"
     ]
    }
   ],
   "source": [
    "print(coactive_cells_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 14, 18, 19, 20, 21, 22, 25, 26, 32, 34, 35, 37, 38, 42, 48, 50, 53, 56, 59, 64, 72, 74, 78, 81, 83, 90, 91, 92, 93, 94, 95, 97, 112, 117, 119, 129, 132, 133, 136, 139, 140, 141, 143, 146, 154, 155, 170, 172, 174, 182, 187, 192, 197, 198, 207, 212, 216, 220, 226, 231, 263, 265, 271, 281, 284, 285, 289, 293, 311, 316, 320, 327, 335, 354, 372, 374, 376, 416, 453, 481, 486, 496, 501, 503, 518, 529, 535, 554, 599, 654, 753, 756, 765, 773, 866, 1069, 1367, 1651]\n",
      "[16, 1, 24, 5, 43, 11, 9, 50, 70, 61, 89, 132, 37, 64, 79, 68, 156, 98, 177, 526, 378, 13, 66, 53, 18, 60, 120, 81, 88, 34, 74, 189, 598, 374, 386, 135, 230, 84, 123, 178, 481, 136, 410, 143, 105, 30, 190, 77, 125, 126, 257, 150, 107, 295, 181, 222, 325, 254, 278, 380, 812, 830, 590, 554, 579, 477, 448, 1134, 375, 39, 538, 259, 137, 237, 452, 719, 383, 508, 451, 679, 284, 292, 303, 41, 309, 102, 366, 408, 655, 186, 1033, 314, 931, 484, 273, 180, 290, 1318, 1052]\n"
     ]
    }
   ],
   "source": [
    "print(matched_active_1)\n",
    "print(matched_active_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary Additions:\n",
    "\n",
    "#  Add a section to the post dictionary entitled 'matches' containing the array of matched cells.  \n",
    "# Don't need more than this as you can work everything out from these. \n",
    "\n",
    "for cell_1, cell_2 in zip(cell_dict_1.keys(), cell_dict_2.keys()):\n",
    "    cell_dict_1[cell_1]['matched_cells'] = cell_array\n",
    "    cell_dict_2[cell_2]['matched_cells'] = cell_array\n",
    "\n",
    "with open(BASE_PATH_1+'cells.pkl','wb') as f:\n",
    "        pickle.dump(cell_dict_1,f)\n",
    "\n",
    "with open(BASE_PATH_2+'cells.pkl','wb') as f:\n",
    "        pickle.dump(cell_dict_2,f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
